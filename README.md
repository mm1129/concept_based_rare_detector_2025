# Concept-based Explainable Data Mining with VLM for 3D Detection

A cross-modal dataâ€mining pipeline that leverages 2D Visionâ€“Language Models (VLMs) to mine rare and safety-critical objects from driving scenes, and uses them to improve 3D object detection performance on point-cloud data.  See details for "paper" folder.
Implementation is in "projects/detect_rare_images". Main implementation: projects/detect_rare_images/vision-language-model-project


**Key features**  
- **Object Concept Embedding**:  
  - 2D object detection and cropping with YOLOv8  
  - CLIPâ€based semantic feature extraction  
  - Outlier detection via combined Isolation Forest + t-SNE  
- **Conceptâ€guided Filtering**:  
  - Visionâ€“Language captioning (Qwen2-VL-2B-Instruct)  
  - Cosine similarity to predefined concept vocabulary  
  - Three-level grouping: _Common_, _Rare_, _Target_  
- **Targeted Data Mining**:  
  - Scenes containing â€œTargetâ€ concepts (e.g. motorcycle, bicycle)  
  - Scenes containing â€œRareâ€ concepts not in common class list  
  - Build training splits (â€œRandom 10% + Target 10%â€)  
- **Efficiency & Explainability**:  
  - Achieves 80% of fullâ€data performance using only 20% of nuScenes  
  - Transparent selection via captionâ€based concept scores & t-SNE visualizations  
<img width="441" height="261" alt="image" src="https://github.com/user-attachments/assets/3002008c-93eb-48bd-a9c6-c51636108e53" />
<img width="624" height="289" alt="image" src="https://github.com/user-attachments/assets/89e869aa-109b-4c80-9c05-2f957582c1d4" />
<img width="634" height="550" alt="image" src="https://github.com/user-attachments/assets/0f0042cb-abe7-4a24-a795-6a12cd928f57" />

Detected images including rare objects (detected from nuScenes[1] training set)
<img width="386" height="463" alt="image" src="https://github.com/user-attachments/assets/978c6fdd-cbbc-424a-87c5-0edb0edb8281" />

Citations:
[1] Caesar, Holger, et al. "nuscenes: A multimodal dataset for autonomous driving." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.

## Disclaimer
This code is released for academic reproducibility only. It is provided "as is" without any guarantee of support or maintenance.  
Issues may not be actively responded to. Use at your own risk.


---
# vision-language-model-project/vision-language-model-project/README.md

# Vision Language Model Project

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ç”»åƒã¨è¨€èªã®çµ±åˆã‚’ç›®çš„ã¨ã—ãŸãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«ã€CLIPFeatureExtractorã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚„ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã—ã€VLMã‚’åˆ©ç”¨ã—ã¦ç‰¹å¾´é‡ã¨ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚

## æ§‹æˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹æˆã«ãªã£ã¦ã„ã¾ã™ï¼š

- `src/`: ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
  - `models/`: ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®ã‚³ãƒ¼ãƒ‰ã€‚
    - `clip_extractor.py`: CLIPFeatureExtractorã‚¯ãƒ©ã‚¹ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚
    - `vlm_processor.py`: VLMã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã¨ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã¾ãŸã¯é–¢æ•°ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚
  - `utils/`: ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
    - `image_processing.py`: ç”»åƒå‡¦ç†ã«é–¢é€£ã™ã‚‹é–¢æ•°ã€‚
  - `inference/`: æ¨è«–é–¢é€£ã®ã‚³ãƒ¼ãƒ‰ã€‚
    - `captioning.py`: ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã«é–¢é€£ã™ã‚‹é–¢æ•°ã‚„ã‚¯ãƒ©ã‚¹ã€‚
- `config/`: ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã€‚
  - `model_config.json`: ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’JSONå½¢å¼ã§ä¿å­˜ã€‚
- `main.py`: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚
- `requirements.txt`: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¾å­˜é–¢ä¿‚ã€‚

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

```
pip install -r requirements.txt
```

## ä½¿ç”¨æ–¹æ³•

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€`main.py`ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å…·ä½“çš„ãªä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚


## Usage memo
```
docker run --gpus all -d --shm-size=64g -v $PWD/:/workspace -v $(realpath ../data_nuscenes/nuscenes):/workspace/data_nuscenes blip bash -c "python projects/detect_rare_images/vision-language-model-project/main.py

```
[examples]
```
root@51b6be5b3808:/workspace# python projects/detect_rare_images/vision-language-model-project/main.py --max-images 200
ãƒ—ãƒ­ãƒƒãƒˆã‚’è‹±èªè¡¨ç¤ºã«è¨­å®šã—ã¾ã—ãŸ
çµæœã¯ ./outlier_detection_results/outlier_detection_0502_12-27-49_JST ã«ä¿å­˜ã•ã‚Œã¾ã™
è¨­å®šæƒ…å ±: {'images_folder': './data_nuscenes/samples/CAM_FRONT', 'output_dir': './outlier_detection_results', 'qwen_model_size': '2B', 'contamination': 0.1, 'target_classes': ['construction_vehicle', 'bicycle', 'motorcycle', 'trailer', 'truck'], 'common_classes': ['car', 'pedestrian', 'traffic_light', 'traffic_sign'], 'save_crops': True, 'save_descriptions': True, 'save_probability_plots': True, 'cleanup_temp_files': False, 'max_images': 200, 'seed': 42, 'timestamp': '0502_12-27-49_JST', 'device': 'cuda:0', 'sampling_rate': 0.2, 'skip_visualization': True, 'skip_caption': True, 'parallel': False, 'workers': None, 'use_blip': True, 'process_all_blip': False, 'weight_text': 0.5}
å‡¦ç†å¯¾è±¡: 200æšã®ç”»åƒï¼ˆæœ€å¤§æ•°åˆ¶é™ï¼‰
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨: 200æš â†’ 40æš (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: 0.2%)
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¨åˆ‡ã‚ŠæŠœãã‚’å®Ÿè¡Œä¸­...
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºé€²æ—: 2.5% (1/40) | çµŒé: 0.6ç§’ | æ®‹ã‚Š: 0.0ç§’
Ultralytics YOLOv8.0.20 ğŸš€ Python-3.10.14 torch-2.2.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)
YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºé€²æ—: 100.0% (40/40) | çµŒé: 7.7ç§’ | æ®‹ã‚Š: 0.0ç§’
æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: 231
æ—©æœŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: 231å€‹ â†’ 46å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
ç‰¹å¾´æŠ½å‡ºã‚’å®Ÿè¡Œä¸­...
ç‰¹å¾´æŠ½å‡ºé€²æ—: 69.6% (32/46) | çµŒé: 0.0ç§’ | æ®‹ã‚Š: 0.0ç§’
ç‰¹å¾´æŠ½å‡ºé€²æ—: 100.0% (46/46) | çµŒé: 0.2ç§’ | æ®‹ã‚Š: -0.0ç§’
æœ‰åŠ¹ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ•°: 46/46
t-SNEå¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™
IsolationForestã«ã‚ˆã‚‹ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºã‚’å®Ÿè¡Œä¸­ (contamination=0.1)...
IsolationForestãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­ (contamination=0.1)...
LOFã«ã‚ˆã‚‹ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºã‚’å®Ÿè¡Œä¸­ (contamination=0.1)...
Isolation Forestã¨LOFã®çµæœã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ (union)...
æ¤œå‡ºçµæœ: Isolation Forest 5ä»¶, LOF 5ä»¶, çµ„ã¿åˆã‚ã› 8ä»¶
å‡¦ç†å¯¾è±¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: 46 (ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢: 8, å¸Œå°‘ã‚¯ãƒ©ã‚¹: 46)
ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŸã‚ãƒ¢ãƒ‡ãƒ«ã¯åˆæœŸåŒ–ã—ã¾ã›ã‚“
ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã¨æœ€çµ‚åˆ¤å®šã‚’å®Ÿè¡Œä¸­...
ã‚µãƒ–ãƒãƒƒãƒå‡¦ç†ä¸­: 1ã€œ46/46...
ã‚¯ãƒ©ã‚¹åˆ¥ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...
å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ å‡¦ç†æ™‚é–“: 15.4ç§’
çµæœã¯ ./outlier_detection_results/outlier_detection_0502_12-27-49_JST ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚
root@51b6be5b3808:/workspace# python projects/detect_rare_images/vision-language-model-project/main.py --max-images 200
ãƒ—ãƒ­ãƒƒãƒˆã‚’è‹±èªè¡¨ç¤ºã«è¨­å®šã—ã¾ã—ãŸ
çµæœã¯ ./outlier_detection_results/outlier_detection_0502_12-31-29_JST ã«ä¿å­˜ã•ã‚Œã¾ã™
è¨­å®šæƒ…å ±: {'images_folder': './data_nuscenes/samples/CAM_FRONT', 'output_dir': './outlier_detection_results', 'qwen_model_size': '2B', 'contamination': 0.1, 'target_classes': ['construction_vehicle', 'bicycle', 'motorcycle', 'trailer', 'truck'], 'common_classes': ['car', 'pedestrian', 'traffic_light', 'traffic_sign'], 'save_crops': True, 'save_descriptions': True, 'save_probability_plots': True, 'cleanup_temp_files': False, 'max_images': 200, 'seed': 42, 'timestamp': '0502_12-31-29_JST', 'device': 'cuda:0', 'sampling_rate': 0.2, 'skip_visualization': True, 'skip_caption': True, 'parallel': False, 'workers': None, 'use_blip': True, 'process_all_blip': False, 'weight_text': 0.25}
å‡¦ç†å¯¾è±¡: 200æšã®ç”»åƒï¼ˆæœ€å¤§æ•°åˆ¶é™ï¼‰
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨: 200æš â†’ 40æš (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡: 0.2%)
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã¨åˆ‡ã‚ŠæŠœãã‚’å®Ÿè¡Œä¸­...
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºé€²æ—: 2.5% (1/40) | çµŒé: 0.7ç§’ | æ®‹ã‚Š: 0.0ç§’
Ultralytics YOLOv8.0.20 ğŸš€ Python-3.10.14 torch-2.2.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)
YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs
ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºé€²æ—: 100.0% (40/40) | çµŒé: 5.8ç§’ | æ®‹ã‚Š: 0.0ç§’
æ¤œå‡ºã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: 231
æ—©æœŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: 231å€‹ â†’ 46å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
ç‰¹å¾´æŠ½å‡ºã‚’å®Ÿè¡Œä¸­...
ç‰¹å¾´æŠ½å‡ºé€²æ—: 69.6% (32/46) | çµŒé: 0.0ç§’ | æ®‹ã‚Š: 0.0ç§’
ç‰¹å¾´æŠ½å‡ºé€²æ—: 100.0% (46/46) | çµŒé: 0.2ç§’ | æ®‹ã‚Š: -0.0ç§’
æœ‰åŠ¹ãªç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ•°: 46/46
t-SNEå¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™
IsolationForestã«ã‚ˆã‚‹ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºã‚’å®Ÿè¡Œä¸­ (contamination=0.1)...
IsolationForestãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­ (contamination=0.1)...
LOFã«ã‚ˆã‚‹ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºã‚’å®Ÿè¡Œä¸­ (contamination=0.1)...
Isolation Forestã¨LOFã®çµæœã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ (union)...
æ¤œå‡ºçµæœ: Isolation Forest 5ä»¶, LOF 5ä»¶, çµ„ã¿åˆã‚ã› 8ä»¶
å‡¦ç†å¯¾è±¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°: 46 (ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢: 8, å¸Œå°‘ã‚¯ãƒ©ã‚¹: 46)
ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãŸã‚ãƒ¢ãƒ‡ãƒ«ã¯åˆæœŸåŒ–ã—ã¾ã›ã‚“
ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã¨æœ€çµ‚åˆ¤å®šã‚’å®Ÿè¡Œä¸­...
ã‚µãƒ–ãƒãƒƒãƒå‡¦ç†ä¸­: 1ã€œ46/46...
ã‚¯ãƒ©ã‚¹åˆ¥ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...
å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ å‡¦ç†æ™‚é–“: 13.5ç§’
çµæœã¯ ./outlier_detection_results/outlier_detection_0502_12-31-29_JST ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚
root@51b6be5b3808:/workspace# 
```

