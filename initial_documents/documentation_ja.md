# 概念ベース希少物体検出システム - 技術ドキュメント

## 1. システム概要

このシステムは自動運転車や監視カメラなどで撮影された画像から、一般的でない「珍しい物体」や「異常物体」を自動検出するための機械学習システムです。特に以下の特徴を持っています：

1. **2段階の物体認識アプローチ**：
   - YOLOv8による物体検出と抽出
   - マルチモーダルモデル（CLIP、Qwen2-VL）による意味解析

2. **異常検知の多層的アプローチ**：
   - 特徴空間での異常検知（Isolation Forest）
   - 意味的概念による異常判定（テキスト理解と概念マッチング）

3. **高精度な物体理解**：
   - CLIPによる効率的な画像特徴抽出
   - 大規模ビジョン言語モデル（VLM）によるコンテキスト理解

4. **実用性を考慮した設計**：
   - バッチ処理によるメモリ効率と高速処理
   - GPUリソースに適応するダイナミックなバッチサイズ調整

## 2. アーキテクチャと主要コンポーネント

### 2.1 物体検出とセグメンテーション（DetectorYOLO）

```python
class DetectorYOLO:
    def __init__(self, model_path="yolov8l.pt", device=None):
        # ...
    
    def detect_and_crop(self, image_path, out_dir="cropped_objects", conf_thres=0.3):
        # ...
```

**実装の特徴と工夫点：**

- **効率的な画像前処理**：
  - 最大サイズ（1280px）への動的リサイズによる処理効率向上
  - 元解像度との変換比率を保持し、検出結果を正確に元画像に適用

- **大量オブジェクト処理の最適化**：
  - 1画像あたり最大50オブジェクトに制限し、重要でないオブジェクトを除外
  - 信頼度スコアによる優先順位付けで重要なオブジェクトを優先処理

- **ロバスト性向上**：
  - 複数の例外処理による安定した実行保証
  - 無効な画像パスや破損ファイルへの対策

### 2.2 特徴抽出（CLIPFeatureExtractor）

```python
class CLIPFeatureExtractor:
    def __init__(self, model_name="openai/clip-vit-base-patch32", device=None):
        # ...
    
    def get_image_embedding(self, image_path):
        # ...
    
    def get_image_embeddings_batch(self, image_paths, batch_size=32):
        # ...
    
    def get_text_embedding(self, text_list):
        # ...
```

**実装の特徴と工夫点：**

- **効率的なバッチ処理**：
  - 画像を一度に処理することで計算効率を大幅に向上
  - 進捗表示による長時間処理の可視化

- **メモリ管理の最適化**：
  - バッチ処理後の不要なテンソルの積極的な解放
  - GPUメモリキャッシュのクリアによるメモリリーク防止

- **エラー耐性**：
  - 破損画像や処理エラーに対応した堅牢な設計
  - 結果配列の整合性を維持するためのNone値挿入

- **マルチモーダル機能**：
  - 画像だけでなくテキスト特徴も同じモデルで抽出可能
  - 画像とテキスト間の意味的関連付けを可能にする設計

### 2.3 テキスト分析と概念マッチング

```python
def parse_text_with_probability(base_text, candidate_labels, clip_extractor):
    # テキストとラベル候補のコサイン類似度を計算し、確率分布として返す
```

**実装の特徴と工夫点：**

- **効率的な意味的マッチング**：
  - CLIPのテキストエンコーダを活用した高速なセマンティック分析
  - ベクトル正規化によるコサイン類似度計算の最適化

- **確率分布への変換**：
  - 類似度スコアを活用した柔軟な概念マッチング
  - 降順ソートによる高い類似度を持つ概念の優先表示

- **ゼロ以上の類似度のみ抽出**：
  - 負の類似度（逆相関）を持つ概念を除外し、意味的に関連する概念のみに注目

### 2.4 画像キャプション生成（Qwen2-VL）

```python
def generate_descriptions_batch(image_paths, model, processor, batch_size=8, detected_classes=None):
    # バッチ処理で画像キャプションを生成
```

**実装の特徴と工夫点：**

- **動的バッチサイズ調整**：
  - GPUメモリ利用状況に基づく最適なバッチサイズの自動決定
  - メモリ不足エラーを回避しつつ最大のスループットを実現

- **プロンプトエンジニアリング**：
  - 運転安全性に関連する特化したプロンプト設計
  - YOLOの検出結果をヒントとして提供し、より正確な解釈を促進

- **リソース効率化**：
  - 混合精度演算（FP16）の活用による処理速度向上とメモリ使用量削減
  - サブバッチ処理と定期的なメモリクリアによる長時間安定処理の実現

- **詳細な進捗管理**：
  - 残り時間予測を含む精密な進捗表示
  - GPUメモリ使用状況の監視と報告

### 2.5 異常検知（Isolation Forest）

```python
def train_isolation_forest(features, contamination=0.1):
    # 特徴ベクトルからIsolation Forestモデルを学習

def predict_outliers(features, iso_model):
    # 事前学習したモデルを使用して異常を検出
```

**実装の特徴と工夫点：**

- **教師なし学習アプローチ**：
  - ラベル不要の異常検知手法による未知の異常の検出
  - 汚染率（contamination）パラメータによる検出感度の調整

- **高次元特徴空間での異常検知**：
  - CLIPの512次元特徴空間での効率的な異常検知
  - ランダム森による計算効率の高い実装

- **安全性チェック**：
  - 空の特徴リストに対する堅牢な処理
  - 一貫した乱数シードによる再現性の確保

### 2.6 可視化（t-SNE）

```python
def visualize_tsne(features, labels=None, title="t-SNE Visualization"):
    # 高次元特徴の2次元可視化を生成
```

**実装の特徴と工夫点：**

- **適応的パラメータ設定**：
  - データサイズに応じたperplexityパラメータの自動調整
  - 少数サンプルでも機能する堅牢な設計

- **クラス別可視化**：
  - ラベルに基づいた色分け表示によるパターン把握の容易化
  - 凡例要素の自動生成による直感的理解の促進

- **ファイル自動保存**：
  - 標準化されたファイル名生成と保存ディレクトリ管理
  - 後続分析のための高品質な可視化出力

### 2.7 メモリ最適化とリソース管理

```python
def get_optimal_batch_size(initial_size=8, min_size=1):
    # 利用可能なGPUメモリに基づいて最適なバッチサイズを決定
```

**実装の特徴と工夫点：**

- **動的リソース割り当て**：
  - 現在のGPUメモリ状態に基づく適応的バッチサイズ決定
  - メモリ不足時の段階的なバッチサイズ縮小

- **最小バッチサイズの保証**：
  - 極端に小さいバッチサイズによる非効率な処理を回避
  - CPU環境でも機能する堅牢な実装

- **例外処理**：
  - GPUメモリ取得エラーに対する適切なフォールバック
  - デバイス非依存の設計による様々な環境での動作保証

## 3. 主要機能とワークフロー

### 3.1 単一フォルダ処理モード（detect_outliers_single_folder）

```python
def detect_outliers_single_folder(
    images_folder,
    output_dir,
    qwen_model_size="2B",
    contamination=0.1,
    target_classes=None,
    common_classes=None,
    # ...
):
    # 単一フォルダ内の画像からアウトライアを検出する関数
```

このモードは、1つのフォルダに含まれる画像をすべて処理し、異常オブジェクトを検出します。

**処理ステップ：**

1. 設定の初期化と出力ディレクトリの作成
2. 画像の検出とオブジェクト切り抜き（YOLOv8）
3. 特徴抽出（CLIP）
4. t-SNEによる特徴空間の可視化
5. Isolation Forestによる異常検知
6. 特定クラスオブジェクトの抽出
7. キャプション生成（Qwen2-VL）
8. テキスト解析と概念マッチング
9. 結果の保存と整理

**特徴と工夫点：**

- **完全自動化**：
  - 単一コマンドですべての処理を実行
  - 設定ファイルの自動保存による再現性確保

- **処理の透明性**：
  - 詳細な進捗表示と時間予測
  - 複数の可視化出力による結果解釈の支援

- **フレキシブルな設定**：
  - 多様なパラメータによる検出感度の調整
  - 特定クラスへの注目機能

- **効率的なリソース管理**：
  - 一時ファイルのクリーンアップオプション
  - 処理数制限による実験的利用の効率化

### 3.2 ベースライン学習と新規画像検出（train_baseline_model, detect_new_images）

このモードでは、「正常」サンプルからベースラインモデルを学習し、新規画像に対して異常検出を行います。

```python
def train_baseline_model(baseline_folder, qwen_model_size, use_concept_list, concept_list, main_out_dir, lim1=0.8, train_indices=None):
    # ベースラインデータからモデルを学習

def detect_new_images(new_images_folder, clip_extractor, candidate_labels, qwen_model_size, main_out_dir, common_classes, ...):
    # 学習済みモデルを使用して新規画像から異常を検出
```

**処理ステップ：**

1. ベースラインデータからの特徴抽出と異常検知モデル学習
2. 概念リストの生成または拡張
3. 新規画像の処理と特徴抽出
4. ベースラインとの比較による異常スコア計算
5. 結果の可視化と保存

**特徴と工夫点：**

- **2段階の異常検知**：
  - 特徴空間での異常検知と意味的概念での異常検知の組み合わせ
  - 複数の検出基準による高精度な判定

- **コンセプト拡張機能**：
  - 検出クラスからの自動的な候補ラベル生成
  - VLMを用いた概念拡張による検出カバレッジの向上

- **メモリ効率の高いバッチ処理**：
  - サブバッチ分割による大規模データセット処理の実現
  - 動的なメモリ管理による安定処理

## 4. システムの技術的工夫と最適化

### 4.1 大規模データ処理の最適化

- **進行的バッチ処理**：
  - 膨大な画像を小さなバッチに分割して処理
  - メモリ使用量の監視と管理

- **マルチステージパイプライン**：
  - 各処理段階（検出、特徴抽出、キャプション生成）を分離
  - 段階間でのメモリ解放による効率化

- **並列処理の活用**：
  - GPUの並列計算能力を最大限活用するバッチ設計
  - バッチサイズの動的調整による処理効率の最適化

### 4.2 メモリ管理の工夫

- **積極的なメモリ解放**：
  - 不要になったテンソルやモデルの明示的解放
  - 定期的なGPUキャッシュクリア

- **混合精度計算**：
  - 適切な箇所でのFP16計算による高速化とメモリ削減
  - 大規模モデル推論の効率化

- **リソース監視**：
  - メモリ使用状況のモニタリングと報告
  - リソース不足に対応する動的調整メカニズム

### 4.3 ロバスト性と例外処理

- **多層的なエラーハンドリング**：
  - 関数レベル、ループレベル、バッチレベルでの例外捕捉
  - エラーログ記録と処理継続の保証

- **結果整合性の維持**：
  - エラー発生時のデータ構造整合性保持メカニズム
  - 不完全な結果に対する適切な処理

- **実行状態の保存**：
  - 設定と実行状況の詳細記録
  - 中断や異常終了からの回復可能性

## 5. 拡張性と将来の発展

### 5.1 モデル拡張性

- **モジュール化された設計**：
  - 各コンポーネント（検出器、特徴抽出器、キャプション生成器）の独立実装
  - 新しいモデルへの容易な置き換えが可能

- **モデルサイズのオプション**：
  - 計算リソースに応じたモデルサイズの選択肢（2B/7B）
  - 軽量モデルから高性能モデルまでのスケーラビリティ

### 5.2 将来の発展方向

- **マルチモーダル統合の強化**：
  - 画像、テキスト、メタデータの統合的解析
  - 複数のモダリティからの証拠融合

- **時間的コンテキストの活用**：
  - ビデオストリームにおける時間的文脈の考慮
  - 前後のフレーム情報を活用した判断精度向上

- **説明可能性の向上**：
  - 検出結果の根拠をより詳細に提示する機能
  - 異常判定の理由付けとビジュアライゼーション

## 6. 実装上の注意点

- **メモリ要件**：
  - 最小8GB GPU RAM推奨（Qwen2-VL-2B使用時）
  - 16GB以上のGPU RAMが理想的（大規模バッチ処理時）

- **依存ライブラリ**：
  - PyTorch 2.0以上
  - Transformers 4.30以上
  - Ultralytics（YOLOv8）

- **計算コスト**：
  - 1,000枚の画像処理に約2-4時間（GPU環境、画像内容による）
  - モデルサイズと処理対象に応じた計算リソースの確保が必要 